{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "necessary-cotton",
   "metadata": {},
   "source": [
    "# Word2Vec Model\n",
    "Word2Vec Google's Pretrained Model\n",
    "Contains vector representations of 50 billion words\n",
    "\n",
    "Words which are similar in context have similar vectors\n",
    "\n",
    "Distance/Similarity between two words can be measured using Cosine Distance\n",
    "\n",
    "## Applications\n",
    "\n",
    "Text Similarity\n",
    "Language Translation\n",
    "Finding Odd Words\n",
    "Word Analogies\n",
    "Word Embeddings\n",
    "Word embeddings are numerical representation of words, in the form of vectors.\n",
    "\n",
    "Word2Vec Model represents each word as 300 Dimensional Vector\n",
    "\n",
    "In this tutorial we are going to see how to use pre-trained word2vec model.\n",
    "\n",
    "Model size is around 1.5 GB\n",
    "We will work using Gensim, which is popular NLP Package.\n",
    "Gensim's Word2Vec Model provides optimum implementation of\n",
    "\n",
    "1) CBOW Model\n",
    "\n",
    "2) SkipGram Model\n",
    "\n",
    "Paper 1 Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "Paper 2 Distributed Representations of Words and Phrases and their Compositionality\n",
    "\n",
    "### Word2Vec using Gensim\n",
    "Link https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "CODE\n",
    "Load Word2Vec Model\n",
    "KeyedVectors - This object essentially contains the mapping between words and embeddings. After training, it can be used directly to query those embeddings in various ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "periodic-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "directed-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_apple = word_vectors[\"apple\"] \n",
    "v_mango = word_vectors[\"india\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developed-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(v_apple.shape)\n",
    "print(v_mango.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wrapped-abuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17158598]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([v_mango],[v_apple])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confirmed-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-headline",
   "metadata": {},
   "source": [
    "## Find the Odd One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impressed-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_one_out(words):\n",
    "    \"\"\"Accepts a list of words and returns the odd word\"\"\"\n",
    "    \n",
    "    # Generate all word embeddings for the given list\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\n",
    "    avg_vector = np.mean(all_word_vectors,axis=0)\n",
    "    print(avg_vector.shape)\n",
    "    \n",
    "    #Iterate over every word and find similarity\n",
    "    odd_one_out = None\n",
    "    min_similarity = 1.0 #Very high value\n",
    "    \n",
    "    for w in words:\n",
    "        sim = cosine_similarity([word_vectors[w]],[avg_vector])\n",
    "        if sim < min_similarity:\n",
    "            min_similarity = sim\n",
    "            odd_one_out = w\n",
    "    \n",
    "        print(\"Similairy btw %s and avg vector is %.2f\"%(w,sim))\n",
    "            \n",
    "    return odd_one_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"] \n",
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]        \n",
    "input_3  = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\n",
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaptive-brunswick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw apple and avg vector is 0.78\n",
      "Similairy btw mango and avg vector is 0.76\n",
      "Similairy btw juice and avg vector is 0.71\n",
      "Similairy btw party and avg vector is 0.36\n",
      "Similairy btw orange and avg vector is 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'party'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "revolutionary-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw music and avg vector is 0.66\n",
      "Similairy btw dance and avg vector is 0.81\n",
      "Similairy btw sleep and avg vector is 0.51\n",
      "Similairy btw dancer and avg vector is 0.72\n",
      "Similairy btw food and avg vector is 0.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "meaning-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw match and avg vector is 0.58\n",
      "Similairy btw player and avg vector is 0.68\n",
      "Similairy btw football and avg vector is 0.72\n",
      "Similairy btw cricket and avg vector is 0.70\n",
      "Similairy btw dancer and avg vector is 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dancer'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fitting-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw india and avg vector is 0.81\n",
      "Similairy btw paris and avg vector is 0.75\n",
      "Similairy btw russia and avg vector is 0.79\n",
      "Similairy btw france and avg vector is 0.81\n",
      "Similairy btw germany and avg vector is 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-terry",
   "metadata": {},
   "source": [
    "## Word Analogies Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-huntington",
   "metadata": {},
   "source": [
    "man -> woman ::    prince -> princess\n",
    "italy -> italian ::    spain -> spanish\n",
    "india -> delhi ::  japan -> tokyo\n",
    "man -> woman ::    boy -> girl\n",
    "small -> smaller ::    large -> larger\n",
    "\n",
    "Try it out\n",
    "man -> coder :: woman -> ______?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bizarre-graduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[\"man\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "golden-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(a,b,c,word_vectors):\n",
    "    \"\"\"Accepts a triad of words, a,b,c and returns d such that a is to b : c is to d\"\"\"\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    \n",
    "    # similarity |b-a| = |d-c| should be max\n",
    "    max_similarity = -100 \n",
    "    \n",
    "    d = None\n",
    "    \n",
    "    words = word_vectors.vocab.keys()\n",
    "    \n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    \n",
    "    #to find d s.t similarity(|b-a|,|d-c|) should be max\n",
    "    \n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "            \n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "documented-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "#triad_2 = (\"man\",\"woman\",\"prince\")\n",
    "#predict_word(*triad_2,word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-recognition",
   "metadata": {},
   "source": [
    "## Using the Most Similar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "charitable-plant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-sister",
   "metadata": {},
   "source": [
    "## Training Your Own Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-argentina",
   "metadata": {},
   "source": [
    "Word2Vec model can learn embeddings from any text corpus!\n",
    "\n",
    "Continuous Bag of Words Model\n",
    "Skip Gram Model\n",
    "Algorithm looks at window of target word(Y) to provide context word(X), the model is trained on (X,Y) pairs in a superwised manner. The algorithm was developed by Tomas Mikolov.\n",
    "\n",
    "Data Preparation\n",
    "Each sentence must be tokenized, into a list of words.\n",
    "\n",
    "The sentences can be text loaded into memory once, or we can build a data pipeline which iteratively feeds data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mounted-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "double-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw  = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "recent-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file \n",
    "def readFile(file): \n",
    "    f = open(file,'r',encoding='utf-8')\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        words =  nltk.word_tokenize(sent)\n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stopw]\n",
    "        data.append(words)\n",
    "        \n",
    "    return data\n",
    "\n",
    "text = readFile('bollywood.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aggregate-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018'], ['the', 'deepika', 'ranveer', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple'], ['from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'deepika', 'ranveer', 'wedding', 'style', 'file'], ['not', 'ambanis', 'deepika', 'ranveer', 'priyanka', 'nick'], ['man', 'proves', 'wedding', 'the', 'year', 'this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['priyanka', 'also', 'shared', 'video', 'featuring', 'nick', 'jonaswas', 'also', 'celebrating', 'the', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'priyanka', 'chopra', 'nick', 'jonas', 'new', 'year', 'celebrations', 'outstanding'], ['priyanka', 'chopra', 'nick', 'shared', 'glimpses', 'celebration', 'verbier', 'switzerland'], ['priyanka', 'chopra', 'married', 'nick', 'jonas', 'december', 'three', 'wedding', 'receptions', 'one', 'new', 'delhi', 'two', 'mumbai'], ['this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['kapil', 'sharma', 'ginni', 'chatrath', 'jaggo', 'night', 'december', 'made', 'even', 'special', 'industry', 'friends'], ['kapil', 'sharma', 'ginni', 'chatrath', 'friends', 'long', 'time'], ['there', 'virat', 'side', 'actress', 'wife', 'anushka', 'sharma', 'pleasure', 'audience'], ['while', 'couple', 'rang', 'new', 'year', 'style', 'morning', 'saw', 'virat', 'dress', 'squad', 'attire', 'anushka', 'pink', 'salwar', 'suit'], ['isha', 'ambani', 'married', 'anand', 'piramal', 'year']]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wired-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "therapeutic-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text,window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adverse-ancient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=116, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accredited-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "pursuant-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'priyanka', 'nick', 'deepika', 'ranveer', 'wedding', 'the', 'chopra', 'sharma', 'ginni', 'weddings', 'jonas', 'kapil', 'chatrath', 'anand', '2018', 'isha', 'ambani', 'piramal', 'saw', 'from', 'new', 'also', 'man', 'singh', 'padukone', 'virat', 'many', 'grand', 'but', 'nothing', 'beats', 'award', 'media', 'shared', 'anushka', 'style', 'couple', 'two', 'social', 'big', 'fat', 'celebrations', 'this', 'december', 'married', 'friends', 'lavish', 'extravagant', 'one', 'entire', 'parties', 'everything', 'timeline', 'file', 'not', 'ambanis', 'pink', 'events', 'happened', 'reception', 'bollywood', 'squad', 'hooked', 'phones', 'waiting', 'come', 'biggest', 'gave', 'enough', 'reason', 'believe', 'stylish', 'attire', 'dress', 'looks', 'airport', 'side', 'morning', 'jaggo', 'celebration', 'verbier', 'switzerland', 'three', 'receptions', 'delhi', 'mumbai', 'night', 'proves', 'made', 'even', 'special', 'industry', 'long', 'time', 'there', 'glimpses', 'outstanding', 'pictures', 'london', 'rang', 'while', 'audience', 'pleasure', 'wife', 'actress', 'salwar', 'video', 'featuring', 'jonaswas', 'celebrating', 'family', 'first', 'celebrated', 'christmas', 'suit']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acute-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model[\"deepika\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "incorporate-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_vectors = [model[w] for w in  model.wv.get_normed_vectors()]\n",
    "#all_vectors = np.array(all_vectors)\n",
    "#print(all_vectors.shape)\n",
    "#print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "applicable-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.manifold import TSNE\n",
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300,random_state=1)\n",
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300,random_state=1)\n",
    "#tsne = TSNE(n_components=2, verbose=1,n_iter=1000,random_state=1)\n",
    "\n",
    "#tsne_results = tsne.fit_transform(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "brief-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\n",
    "\n",
    "\n",
    "def predict_actor(a,b,c,word_vectors):\n",
    "    \"\"\"Accepts a triad of words, a,b,c and returns d such that a is to b : c is to d\"\"\"\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100 \n",
    "    \n",
    "    d = None\n",
    "    words = actors\n",
    "    \n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    \n",
    "    #to find d s.t similarity(|b-a|,|d-c|) should be max\n",
    "    \n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "matched-sleeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepika'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"nick\",\"priyanka\",\"virat\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "wanted-association",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singh'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"deepika\",\"priyanka\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "solar-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ginni'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"singh\",\"deepika\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
